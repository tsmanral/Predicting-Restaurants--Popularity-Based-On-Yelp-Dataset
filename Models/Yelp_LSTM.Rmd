---
title: "IE 7275 project"
author: "Tribhuwan Singh"
date: "4/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(keras)
library(RSNNS)
library(tensorflow)
library(ggplot2)
library(tidyquant)
library(plotly)
```

```{r}
model_data <- read_csv("C:/Users/tribh/JupyterLabNotebooks/model_data.csv") 
```

```{r}
model_data %>%
    ggplot(aes(X1, weighted_stars)) +
    geom_line(color = palette_light()[[1]], alpha = 0.5) +
    geom_point(color = palette_light()[[1]]) +
    geom_smooth(method = "loess", span = 0.2, se = FALSE) +
    theme_tq() +
    labs(
        title = "Weighted Stars on Original Data changes over the index"
    )
```

```{r}
set.seed(100)
ind <- sample(2, nrow(model_data), replace = TRUE, prob = c(0.7, 0.3))
model_data.training <- model_data[ind == 1, c(1,7,8,9,10,11,12,13)]
model_data.test <- model_data[ind == 2, c(1,7,8,9,10,11,12,13)]

model_data.training %>%
    ggplot(aes(X1, weighted_stars)) +
    geom_line(color = palette_light()[[1]], alpha = 0.5) +
    geom_point(color = palette_light()[[1]]) +
    geom_smooth(method = "loess", span = 0.2, se = FALSE) +
    theme_tq() +
    labs(
        title = "Weighted Stars on Original Training Data changes over the index"
    )

model_data.test %>%
    ggplot(aes(X1, weighted_stars)) +
    geom_line(color = palette_light()[[1]], alpha = 0.5) +
    geom_point(color = palette_light()[[1]]) +
    geom_smooth(method = "loess", span = 0.2, se = FALSE) +
    theme_tq() +
    labs(
        title = "Weighted Stars on Original Test Data changes over the index"
    )
```


```{r}
reshapeDt <- function(data){ # data is the original train matrix (training dataset)
    rows <- nrow(data)
    cols <- ncol(data)
    
    dt <- array(dim=c(rows, 1, cols))
    for(i in 1:rows){
        dt[i,1,] <- data[i,1:cols]
    }
    dt
}
```


```{r}
set.seed(100)
lstm_data1 <- as.matrix(model_data[, c(7:13)])
lstm_data <- normalizeData(lstm_data1, type = "0_1")
colnames(lstm_data) <- colnames(lstm_data1)

```

```{r}
set.seed(100)
ind <- sample(2, nrow(lstm_data), replace = TRUE, prob = c(0.7, 0.3))
lstm.training <- lstm_data[ind == 1, 1:6]
lstm.test <- lstm_data[ind == 2, 1:6]
lstm.trainingtarget <- lstm_data[ind == 1, 7]
lstm.testtarget <- lstm_data[ind == 2, 7]
```


```{r}
model <- keras_model_sequential()

model %>% layer_lstm(units = 128,  input_shape = c(1,6), batch_size = 32, return_sequences = TRUE) %>%
  layer_lstm(units = 64, return_sequences = FALSE) %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 32) %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1)

summary(model)
```

```{r}
model %>% compile(
    optimizer = "rmsprop",
    loss = "mse", 
    metrics = c("mae")
  )
```


```{r}
set.seed(100)
history <- model %>% fit(
  x = reshapeDt(lstm.training),
 y = lstm.trainingtarget,
 epochs = 100,
 validation_split = 0.2,
 callbacks = list(
  callback_model_checkpoint("best_model.h5", monitor='val_loss', mode='min', verbose=1, save_best_only=TRUE),
  callback_early_stopping(monitor='val_loss', mode='min', verbose=1, patience=20),
  callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1)),
 shuffle = FALSE
)
```

```{r}
# load the saved model
saved_model = load_model_hdf5('best_model.h5') 
```

```{r}
train_scores = saved_model %>% evaluate(reshapeDt(lstm.training), lstm.trainingtarget, verbose = 0)
print(train_scores)
```


```{r}
test_scores = saved_model %>% evaluate(reshapeDt(lstm.test), lstm.testtarget, verbose = 0)
print(test_scores)
```



```{r}
y_pred = saved_model %>% predict(reshapeDt(lstm.training))
x_axes = seq(1:length(y_pred))
```


```{r}
y_pred <- as.numeric(y_pred)
lstm.training1 <- as.data.frame(lstm.training)
lstm.training1$Pred <- y_pred
```

```{r}
plot_ly(as.data.frame(lstm.trainingtarget), x = ~x_axes, y = ~as.data.frame(lstm.trainingtarget)[,1], type = "scatter", mode = "markers", color = ~lstm.training1$stars, name = "Actual Data") %>%
 add_trace(y = lstm.training1$Pred, x = x_axes, name = "LSTM Train prediction", mode = "lines")
```


```{r}
y_pred = saved_model %>% predict(reshapeDt(lstm.test))
x_axes = seq(1:length(y_pred))
```

```{r}
y_pred <- as.numeric(y_pred)
lstm.test1 <- as.data.frame(lstm.test)
lstm.test1$Pred <- y_pred
```

```{r}
plot_ly(as.data.frame(lstm.testtarget), x = ~x_axes, y = ~as.data.frame(lstm.testtarget)[,1], type = "scatter", mode = "markers", color = ~lstm.test1$stars, name = "Actual Data") %>%
 add_trace(y = lstm.test1$Pred, x = x_axes, name = "FFN Test prediction", mode = "lines")
```

```{r}
plot(history)
```




























